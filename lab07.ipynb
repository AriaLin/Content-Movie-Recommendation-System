{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: lab07\n",
      "OK, version v1.14.15\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab07.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 5: Regular Expressions, Text Processing\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at **11:59pm Wednesday, May 22**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write names in this cell: Qirong He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "objectives",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Objectives for Lab 5:\n",
    "\n",
    "This lab has two main parts. \n",
    "\n",
    "In the first part, you will practice the basic usage of regular expressions and also learn to use `re` module in Python.  Some of the materials are based on the tutorial at http://opim.wharton.upenn.edu/~sok/idtresources/python/regex.pdf. As you work through the first part of the lab, you may also find the website http://regex101.com helpful. \n",
    "\n",
    "In the second part of the lab, we are going to practice NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Part 1: Regular Expressions\n",
    "\n",
    "We'll start by learning about the simplest possible regular expressions. Since regular expressions are used\n",
    "to operate on strings, we'll start with the most common task: matching characters.\n",
    "\n",
    "Most letters and characters will simply match themselves. For example, the regular expression `r'test'` will match the string `test` exactly. There are exceptions to this rule; some characters are special, and don't match themselves.\n",
    "\n",
    "Here is a list of metacharacters that are widely used in regular experssion. \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >\n",
    "<thead>\n",
    "  <tr style=\"text-align: right;\">\n",
    "    <th>Pattern </th>\n",
    "    <th>Description</th> \n",
    "  </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "  <tr>\n",
    "    <td>^</td>\n",
    "    <td>Matches beginning of line.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$</td>\n",
    "    <td>Matches end of line.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>.</td>\n",
    "    <td>Matches any single character except newline. </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*</td>\n",
    "    <td>Matches 0 or more occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>+</td>\n",
    "    <td>Matches 1 or more occurrence of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>?</td>\n",
    "    <td>Matches 0 or 1 occurrence of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[...]</td>\n",
    "    <td>Matches any single character in brackets.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[^...]</td>\n",
    "    <td>Matches any single character <b>not</b> in brackets.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n}</td>\n",
    "    <td>Matches exactly n number of occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n,}</td>\n",
    "    <td>Matches n or more occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n,m}</td>\n",
    "    <td>Matches at least n and at most m occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>a|b</td>\n",
    "    <td>Matches either a or b.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\1...\\9</td>\n",
    "    <td>Matches n-th grouped subexpression.</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Perhaps the most important metacharacter is the backslash, `\\`. As in Python string literals, the backslash\n",
    "can be followed by various characters to signal various special sequences. It's also used to escape all the\n",
    "metacharacters so you can still match them in patterns; for example, if you need to match a `[` or `\\`, you\n",
    "can precede them with a backslash to remove their special meaning:  `\\[` or `\\\\`. \n",
    "\n",
    "The following predefined special sequences are available:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >\n",
    "<thead>\n",
    "  <tr style=\"text-align: right; font-size=14;\">\n",
    "    <th>Pattern </th>\n",
    "    <th>Description</th> \n",
    "  </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "  <tr>\n",
    "    <td>\\d</td>\n",
    "    <td>Matches any decimal digit; this is equivalent to the class `[0-9]`</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\D</td>\n",
    "    <td>Matches any non-digit character; this is equivalent to the class `[^0-9]`.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\s</td>\n",
    "    <td>Matches any whitespace character; this is equivalent to the class `[ \\t\\n\\r\\f\\v]` </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\S</td>\n",
    "    <td>Matches any non-whitespace character; this is equivalent to the class `[^ \\t\\n\\r\\f\\v]`.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\w</td>\n",
    "    <td>Matches any alphanumeric character; this is equivalent to the class `[a-zA-Z0-9_]`</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\W</td>\n",
    "    <td>Matches any non-alphanumeric character; this is equivalent to the class `[^a-zA-Z0-9_]`.</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 1\n",
    "In this question, write patterns that match the given sequences. It may be as simple as the common letters on each line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 1a\n",
    "\n",
    "Write a single regular expression to match the following strings without using the `|` operator. Notice that the pattern must _start_ with \"abc\".\n",
    "\n",
    "1. **Match:** `abcdefg`\n",
    "1. **Match:** `abcde`\n",
    "1. **Match:** `abc`\n",
    "1. **Skip:** `c abc`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regx1 = r\"^abc[\\w]*\"\n",
    "\n",
    "\n",
    "# fill in your pattern\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 5\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 1b\n",
    "\n",
    "Write a single regular expression to match the following strings without using the `|` operator.\n",
    "\n",
    "1. **Match:** `can`\n",
    "1. **Match:** `man`\n",
    "1. **Match:** `fan`\n",
    "1. **Skip:** `dan`\n",
    "1. **Skip:** `ran`\n",
    "1. **Skip:** `pan`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "regx2 = r\"[^drp]an\" # fill in your pattern\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 7\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 2\n",
    "\n",
    "Now that we have written a few regular expressions, we are now ready to move beyond matching. In this question, we'll take a look at some methods from the `re` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2a:\n",
    "\n",
    "Write a Python program to extract and print the numbers of a given string. \n",
    "\n",
    "1. **Hint:** use `re.findall`\n",
    "2. **Hint:** use `\\d` for digits and one of either `*` or `+`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '20', '30']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_q2a = \"Ten 10, Twenty 20, Thirty 30\"\n",
    "\n",
    "res_q2a = re.findall(r'[0-9]{2}', text_q2a)\n",
    "...\n",
    "\n",
    "res_q2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2b:\n",
    "\n",
    "Write a Python program to replace at most 2 occurrences of space, comma, or dot with a colon.\n",
    "\n",
    "**Hint:** use `re.sub(regex, \"newtext\", string, number_of_occurences)`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python:Exercises: PHP exercises.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_q2b = 'Python Exercises, PHP exercises.'\n",
    "regex= r'[,. ]'\n",
    "number_of_occurences=2\n",
    "res_q2b = re.sub(regex,':', text_q2b,number_of_occurences)\n",
    "... # Hint: use re.sub()\n",
    "...\n",
    "\n",
    "res_q2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2c: \n",
    "\n",
    "Write a Python program to extract values between quotation marks of a string.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'PHP', 'Java']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_q2c = '\"Python\", \"PHP\", \"Java\"'\n",
    "res_q2c = re.findall(r'\"(.*?)\"',text_q2c)\n",
    "...\n",
    "\n",
    "res_q2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2d:\n",
    "\n",
    "Write a regular expression to extract and print the quantity and type of objects in a string. You may assume that a space separates quantity and type, i.e., `\"{quantity} {type}\"`. See the example string below for more detail.\n",
    "\n",
    "1. **Hint:** use `re.findall`\n",
    "2. **Hint:** use `\\d` for digits and one of either `*` or `+`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10 eggs', '20 gooses', '30 giants']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_q2d = \"I've got 10 eggs that I stole from 20 gooses belonging to 30 giants.\"\n",
    "regex=r\"[0-9]{2}.\\w+\"\n",
    "res_q2d = re.findall(regex,text_q2d)\n",
    "...\n",
    "\n",
    "res_q2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2e (optional):\n",
    "\n",
    "Write a regular expression to replace all words that are not `\"mushroom\"` with `\"badger\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'badgerhbadgersbadgerbadgersbadgerbadgerbadgerbadgerorbadgerbadgermushroombadgermushroom'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_qe = 'this is a word mushroom mushroom'\n",
    "\n",
    "regex= r'[^mushroom]'\n",
    "\n",
    "res_qe = re.sub(regex,'badger', text_qe)\n",
    "# Hint: https://www.regextester.com/94017\n",
    "...\n",
    "res_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2f (extra credit):\n",
    "\n",
    "Write a regular expression to replace all words that are `\"US\"` and `\"U.S.\"` with `\"USA\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This will replace USA and USA but not USAGS with USAA.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_qe = 'This will replace US and U.S. but not USGS with USA.'\n",
    "regex= '(U)(\\.)?(S)(\\.)?'\n",
    "\n",
    "res_qe = re.sub(regex,'USA', text_qe,)\n",
    "\n",
    "res_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part 2: NLP\n",
    "\n",
    "Let's reproduce the example of extracting named entities from this article: [Discovering the essential tools for Named Entities Recognition](https://towardsdatascience.com/discovering-the-essential-tools-for-named-entities-recognition-8176c94d9747)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK module\n",
    "import nltk\n",
    "# Import word_tokenize \n",
    "\n",
    "\n",
    "# Import POS tagger\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload content from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # a library for processing webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a request to the website\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Natural_Language_Toolkit\")\n",
    "\n",
    "# Use BeautifulSoup to parse HTML using html5 protocol. It is slower\n",
    "# but more efficient \n",
    "page_content = BeautifulSoup(page.text) #, \"html5lib\")\n",
    "# page_content   # html source of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.[5] NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.[7] NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.[8]NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.[9]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we look for the paragraphs\n",
    "textContent = []\n",
    "for i in range(0, 3):\n",
    "    paragraphs = page_content.find_all(\"p\")[i].text  # find the text inside the paragraph tag <p>\n",
    "    textContent.append(paragraphs)\n",
    "\n",
    "# Join the paragraphs together and replace the `\\n` for empty strings\n",
    "page_text = \" \".join(textContent).replace(\"\\n\", \"\")\n",
    "page_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag and tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create a method that takes the text as an input and use `nltk.word_tokenize` to split the text into tokens. Then, tag each token with its part of speech using `nltk.pos_tag`.\n",
    "\n",
    "The method will return a list of tuples, each consisting of a word along with its tag; the part of the speech that it corresponds to.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DT') ('Natural', 'NNP') ('Language', 'NNP') ('Toolkit', 'NNP') (',', ',')\n",
      "('or', 'CC') ('more', 'JJR') ('commonly', 'RB') ('NLTK', 'NNP') (',', ',')\n",
      "('is', 'VBZ') ('a', 'DT') ('suite', 'NN') ('of', 'IN') ('libraries', 'NNS')\n",
      "('and', 'CC') ('programs', 'NNS') ('for', 'IN') ('symbolic', 'JJ') ('and', 'CC')\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    This function takes a text. Split it in tokens using word_tokenize. \n",
    "    And then tags them using pos_tag from NLTK module.\n",
    "    It outputs a list of tuples. Each tuple consists of a word and the tag with its \n",
    "    part of speech.\n",
    "    \"\"\"\n",
    "    # Get the tokens\n",
    "    tokens =word_tokenize(text)\n",
    "    # Tags the tokens\n",
    "    tagged_tokens =nltk.pos_tag(tokens) \n",
    "    # Returns the list of tuples\n",
    "    return tagged_tokens\n",
    "\n",
    "# Split and label the text\n",
    "label_text = preprocess_text(page_text)\n",
    "for i in range(0,20,5):\n",
    "    print(label_text[i],label_text[i+1],label_text[i+2],label_text[i+3],label_text[i+4])\n",
    "# Print first 20 tuples, 5 per line\n",
    "for i in range(0, 20, 5):\n",
    "   print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3], label_text[i+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2_1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text to get named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform entity detection using a technique called **chunking**.\n",
    "\n",
    "Tokenization extracts only “tokens” or words, whereas, chunking extracts phrases that may have an actual meaning in the text.\n",
    "\n",
    "Chunking requires that our text is first tokenized and POS tagged. It uses these tags as inputs. It outputs “chunks” that can indicate entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can first apply noun pronoun chunks or NP-chunks. We’ll look for chunks matching individual noun phrases. For this, we will customize the regular expressions used in the mechanism.\n",
    "\n",
    "We first need to define rules. They will indicate how sentences should be chunked. You can define your own rules, if you want to extract different chunks.\n",
    "\n",
    "For reference, here's the [list of tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "Our rule states that our NP chunk should consist of an optional determiner (DT) followed by any number of adjectives (JJ) and then one or more pronoun noun (NNP).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP The/DT Natural/NNP Language/NNP Toolkit/NNP)\n",
      "(NP NLTK/NNP)\n",
      "(NP NLP/NNP)\n",
      "(NP English/NNP)\n",
      "(NP the/DT Python/NNP)\n",
      "(NP Steven/NNP Bird/NNP)\n",
      "(NP Edward/NNP Loper/NNP)\n",
      "(NP the/DT Department/NNP)\n",
      "(NP Computer/NNP)\n",
      "(NP Information/NNP Science/NNP)\n",
      "(NP the/DT University/NNP)\n",
      "(NP Pennsylvania/NNP)\n",
      "(NP ]/JJ NLTK/NNP)\n",
      "(NP ]/JJ NLTK/NNP)\n",
      "(NP NLP/NNP)\n",
      "(NP NLTK/NNP)\n",
      "(NP the/DT US/NNP)\n",
      "(NP NLTK/NNP)\n",
      "(NP NLTK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Define the rule\n",
    "rule = \"NP: {<DT>?<JJ>*<NNP>+}\" \n",
    "\n",
    "# We define the parser using the rule\n",
    "parser = nltk.RegexpParser(rule) \n",
    "\n",
    "# Apply to the tagged words \n",
    "# by using the parse() function of the parser you just created\n",
    "# and giving it the label_text\n",
    "result =parser.parse(label_text) \n",
    "\n",
    "# Print only the chunks\n",
    "for entity in result:\n",
    "    if type(entity) == nltk.tree.Tree:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2_2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can instead use a pre-trained classifier using the function `nltk.ne_chunk()`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORGANIZATION Natural/NNP Language/NNP Toolkit/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n",
      "(ORGANIZATION NLP/NNP)\n",
      "(GPE English/NNP)\n",
      "(GPE Python/NNP)\n",
      "(PERSON Steven/NNP Bird/NNP)\n",
      "(PERSON Edward/NNP Loper/NNP)\n",
      "(ORGANIZATION Department/NNP)\n",
      "(ORGANIZATION Computer/NNP)\n",
      "(ORGANIZATION Information/NNP Science/NNP)\n",
      "(ORGANIZATION University/NNP)\n",
      "(GPE Pennsylvania/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n",
      "(ORGANIZATION NLP/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n",
      "(ORGANIZATION US/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n",
      "(ORGANIZATION NLTK/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Use ne_chunk to get entities \n",
    "named_entities =nltk.ne_chunk(label_text)\n",
    "\n",
    "# Print only those that are recognized as entities\n",
    "# Entities have type nltk.tree.Tree\n",
    "for entity in named_entities:\n",
    "    if type(entity) == nltk.tree.Tree:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2_3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the results are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply LDA to classify text in a document to a particular topic [[1]](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925). LDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial.\n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution.\n",
    "\n",
    "To learn more about LDA check out this [link](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another page and see whether LDA will be able to classify their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a request to the website\n",
    "page1 = requests.get(\"https://en.wikipedia.org/wiki/Shallow_parsing\")\n",
    "\n",
    "page1_content = BeautifulSoup(page1.text) #, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shallow parsing (also chunking, \"light parsing\") is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). While the most elementary chunking algorithms simply link constituent parts on the basis of elementary search patterns (e.g. as specified by Regular Expressions), approaches that use machine learning techniques (classifiers, topic modeling, etc.) can take contextual information into account and thus compose chunks in such a way that they better reflect the semantic relations between the basic constituents.[1] That is, these more advanced methods get around the problem that combinations of elementary constituents can have different higher level meanings depending on the context of the sentence.   It is a technique widely used in natural language processing. It is similar to the concept of lexical analysis for computer languages. Under the name of the Shallow Structure Hypothesis, it is also used as an explanation for why second language learners often fail to parse complex sentences correctly.[2] '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we look for the paragraphs\n",
    "text1Content = []\n",
    "for i in range(0, 3):\n",
    "    paragraphs = page1_content.find_all(\"p\")[i].text  # find the text inside the paragraph tag <p>\n",
    "    text1Content.append(paragraphs)\n",
    "\n",
    "# Join the paragraphs together and replace the `\\n` for empty strings\n",
    "page1_text = \" \".join(text1Content).replace(\"\\n\", \"\")\n",
    "page1_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim # another module for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [page_text, page1_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens: use list comprehension\n",
    "    stopped_tokens =[i for i in tokens if i not in stop_words]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(word)for word in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    #texts.append(stopped_tokens)\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shallow',\n",
       " 'pars',\n",
       " 'chunk',\n",
       " 'light',\n",
       " 'pars',\n",
       " 'analysi',\n",
       " 'sentenc',\n",
       " 'identifi',\n",
       " 'constitu',\n",
       " 'part',\n",
       " 'sentenc',\n",
       " 'noun',\n",
       " 'verb',\n",
       " 'adject',\n",
       " 'link',\n",
       " 'higher',\n",
       " 'order',\n",
       " 'unit',\n",
       " 'discret',\n",
       " 'grammat',\n",
       " 'mean',\n",
       " 'noun',\n",
       " 'group',\n",
       " 'phrase',\n",
       " 'verb',\n",
       " 'group',\n",
       " 'elementari',\n",
       " 'chunk',\n",
       " 'algorithm',\n",
       " 'simpli',\n",
       " 'link',\n",
       " 'constitu',\n",
       " 'part',\n",
       " 'basi',\n",
       " 'elementari',\n",
       " 'search',\n",
       " 'pattern',\n",
       " 'e',\n",
       " 'g',\n",
       " 'specifi',\n",
       " 'regular',\n",
       " 'express',\n",
       " 'approach',\n",
       " 'use',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'techniqu',\n",
       " 'classifi',\n",
       " 'topic',\n",
       " 'model',\n",
       " 'contextu',\n",
       " 'inform',\n",
       " 'account',\n",
       " 'compos',\n",
       " 'chunk',\n",
       " 'way',\n",
       " 'better',\n",
       " 'reflect',\n",
       " 'semant',\n",
       " 'relat',\n",
       " 'basic',\n",
       " 'constitu',\n",
       " '1',\n",
       " 'advanc',\n",
       " 'method',\n",
       " 'problem',\n",
       " 'combin',\n",
       " 'elementari',\n",
       " 'constitu',\n",
       " 'differ',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'mean',\n",
       " 'depend',\n",
       " 'context',\n",
       " 'sentenc',\n",
       " 'techniqu',\n",
       " 'wide',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'similar',\n",
       " 'concept',\n",
       " 'lexic',\n",
       " 'analysi',\n",
       " 'languag',\n",
       " 'shallow',\n",
       " 'structur',\n",
       " 'hypothesi',\n",
       " 'explan',\n",
       " 'second',\n",
       " 'languag',\n",
       " 'learner',\n",
       " 'fail',\n",
       " 'pars',\n",
       " 'complex',\n",
       " 'sentenc',\n",
       " 'correctli',\n",
       " '2']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Are stemmed tokens important?\n",
    "\n",
    "Run the analysis below using `stemmed_tokens`, then come back and comment-out the creation of the stemmed tokens in the code above (making sure to properly update `texts`). How does the result change? **Write down your observations and analysis.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_4\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens before stemming are complete which are the same as the original words. However, the tokens after stemming is not complete and get rid of redundant suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text to bag of words and run LDA\n",
    "\n",
    "For topic modelling, we need to convert the preprocessed text to a bag of words, which is a dictionary where the key is a word and the value is the number of times that word occurs in the entire corpus. \n",
    "\n",
    "We then run LDA on our corpus, after we specify how many topics are there in the data set and how many training passes to do over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix (bag-of-words)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25\n",
      "1 32\n",
      "2 5\n",
      "3 6\n",
      "4 7\n",
      "5 8\n",
      "6 9\n",
      "7 accompani\n",
      "8 area\n",
      "9 artifici\n",
      "10 bird\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"25\") appears 1 time.\n",
      "Word 1 (\"32\") appears 1 time.\n",
      "Word 2 (\"5\") appears 1 time.\n",
      "Word 3 (\"6\") appears 1 time.\n",
      "Word 4 (\"7\") appears 1 time.\n",
      "Word 5 (\"8\") appears 1 time.\n",
      "Word 6 (\"9\") appears 1 time.\n",
      "Word 7 (\"accompani\") appears 1 time.\n",
      "Word 8 (\"area\") appears 1 time.\n",
      "Word 9 (\"artifici\") appears 1 time.\n",
      "Word 10 (\"bird\") appears 1 time.\n",
      "Word 11 (\"book\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 0 \n",
    "bow_doc_x = corpus[document_num]\n",
    "\n",
    "num_items = len(bow_doc_x)\n",
    "num_items = 12\n",
    "for i in range(num_items):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '0.043*\"nltk\" + 0.029*\"languag\" + 0.023*\"support\"'), (2, '0.030*\"sentenc\" + 0.030*\"constitu\" + 0.023*\"pars\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.007*\"sentenc\" + 0.007*\"languag\" + 0.007*\"constitu\" + 0.007*\"pars\" + 0.007*\"chunk\"'), (1, '0.043*\"nltk\" + 0.029*\"languag\" + 0.023*\"support\" + 0.016*\"process\" + 0.016*\"inform\"'), (2, '0.030*\"sentenc\" + 0.030*\"constitu\" + 0.023*\"pars\" + 0.023*\"chunk\" + 0.023*\"elementari\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"sentenc\" + 0.007*\"languag\" + 0.007*\"constitu\" + 0.007*\"pars\" + 0.007*\"chunk\" + 0.007*\"elementari\" + 0.007*\"process\" + 0.007*\"verb\" + 0.007*\"part\" + 0.007*\"noun\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.043*\"nltk\" + 0.029*\"languag\" + 0.023*\"support\" + 0.016*\"process\" + 0.016*\"inform\" + 0.016*\"natur\" + 0.016*\"program\" + 0.016*\"tool\" + 0.016*\"univers\" + 0.016*\"nlp\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.030*\"sentenc\" + 0.030*\"constitu\" + 0.023*\"pars\" + 0.023*\"chunk\" + 0.023*\"elementari\" + 0.023*\"languag\" + 0.016*\"analysi\" + 0.016*\"techniqu\" + 0.016*\"mean\" + 0.016*\"part\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We are ready to interpret the model. The output from the model is a list of topics each categorized by a series of words along with the weight of that word in that topic. If you had to come up with the names for the 3 topics that LDA identified, what would you call them?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_5\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0:[Topic One Sentence Word Analysis] \n",
    "We can see all the word has the same weight 0.007, which is samll. Since all the weight is 0.007 we can assume all the word appear together in a sentence. \n",
    "From the meaning we can see the word \"language\" \"sentence\" \"pars\" \"chunk\" and : consituent\", \"group\" indicated that we are going to separte the word from the context to small part. The word \"process\" \"elementary\" \"nature\" and \"nltk\" means we would analyze the word in the sentence. \n",
    "Therefore, put these together we can guess the topic is going to separate the words apart and analysis them using nltk.\n",
    "\n",
    "Topic1：[NLTK base topic]\n",
    "The word “Nltk” has hightest weight, and other words which as less weight such as \"tool\" \"tech\" \"information\" and \"science\" are all related to NLTK Natural Language Toolkit. Therefore we assume the topic is based on NLTK. \n",
    "Topic2:[different words pair topic]\n",
    "The weight of differnet group of words are different (0.030, 0.023,0.016). Therefore we assume words with the same weight appear together. For instance, \"constitu\" \"sentence\" these two words appear together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Congrats! You are finished with this assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**\n",
    "\n",
    "<!-- EXPECT 2 EXPORTED QUESTIONS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n",
      "/opt/conda/lib/python3.6/site-packages/nbconvert/utils/pandoc.py:52: RuntimeWarning: You are using an unsupported version of pandoc (2.7.2).\n",
      "Your version must be at least (1.12.1) but less than (2.0.0).\n",
      "Refer to http://pandoc.org/installing.html.\n",
      "Continuing with doubts...\n",
      "  check_pandoc_version()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lab07.pdf\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving notebook... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR  | auth.py:102 | {'error': 'invalid_grant'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 'lab07.ipynb'.\n",
      "Performing authentication\n",
      "Please enter your bCourses email: h_lin@ucsb.edu\n",
      "\n",
      "Copy the following URL and open it in a web browser. To copy,\n",
      "highlight the URL, right-click, and select \"Copy\".\n",
      "\n",
      "https://okpy.org/client/login/\n",
      "\n",
      "After logging in, copy the code from the web page, paste it below,\n",
      "and press Enter. To paste, right-click and select \"Paste\".\n",
      "\n",
      "Paste your code here: zztM0Lk8QL0IpdJmViAtMpiNuHW6yD\n",
      "Successfully logged in as h_lin@ucsb.edu\n",
      "Submit... 100% complete\n",
      "Submission successful for user: h_lin@ucsb.edu\n",
      "URL: https://okpy.org/ucsb/int15/sp19/lab07/submissions/YWN8gK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "import jassign.to_pdf\n",
    "jassign.to_pdf.generate_pdf('lab07.ipynb', 'lab07.pdf')\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
